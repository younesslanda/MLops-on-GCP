{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe246dac-802e-4041-9cfb-154a53b9813f",
   "metadata": {},
   "source": [
    "Continuous training pipeline with Kubeflow Pipeline and AI Platform\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1843de-e82a-41b9-97fb-04c8be0fe473",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `covertype_training_pipeline.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e405f96-54b7-4207-8f97-2147e8a7cd8a",
   "metadata": {},
   "source": [
    "The pipeline's DSL is to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0913f93a-0624-4e8a-8e66-e84c57c3694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
      "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
      "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
      "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
      "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
      "USE_KFP_SA = os.getenv('USE_KFP_SA')\n"
     ]
    }
   ],
   "source": [
    "!grep 'BASE_IMAGE =' -A 5 pipeline/covertype_training_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2eb65-fe02-4262-acf2-85fc6b19084e",
   "metadata": {},
   "source": [
    "__The custom components execute in a container image defined in base_image/Dockerfile__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c612fa16-b186-4b3f-9c2d-c68fe307ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f42407-076b-454f-b076-3640aef468fc",
   "metadata": {},
   "source": [
    "__The training step in the pipeline employes the AI Platform Training component to schedule a AI Platform Training job in a custom training container. The custom training image is defined in trainer_image/Dockerfile__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4d7c05-0ade-445f-968a-e0b267fffa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]"
     ]
    }
   ],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7743b92-d92a-4098-8583-c0db23ca292d",
   "metadata": {},
   "source": [
    "Building and deploying the pipeline\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524563cc-ea73-461d-83da-94ddc22767ac",
   "metadata": {},
   "source": [
    "Before deploying to AI Platform Pipelines, the pipeline DSL has to be compiled into a pipeline runtime format, also refered to as a pipeline package. The runtime format is based on Argo Workflow, which is expressed in YAML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21523e08-446b-4a96-9bf0-4d7607b95ae6",
   "metadata": {},
   "source": [
    "__Configure environment settings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a375af1f-2f27-47f3-9a91-b6516519d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.zeta-rush-341516.appspot.com/\n",
      "gs://cloud-ai-platform-4aa74d0a-5386-461c-8135-3f0feac88a35/\n",
      "gs://cloud-ai-platform-fffcccf5-f8f6-480b-9bb1-7a0a20be6be1/\n",
      "gs://mlops-youness/\n",
      "gs://storage_bucket_speech/\n",
      "gs://zeta-rush-341516_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7555a-8c1b-4389-94c5-5a31897c1d58",
   "metadata": {},
   "source": [
    "For `ENDPOINT`, we use the value of the host variable in the `Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SDK` section of the SETTINGS window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb4bba-c62f-4596-baee-dee6b1b9d169",
   "metadata": {},
   "source": [
    "For `ARTIFACT_STORE_URI`, we copy the bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cec112-7d75-452a-b1fc-79d4d29acbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = 'https://163ec11eb127b446-dot-us-central1.pipelines.googleusercontent.com' \n",
    "ARTIFACT_STORE_URI = 'gs://mlops-youness'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ba186-3c48-49a3-9fa6-f9e7fbd85340",
   "metadata": {},
   "source": [
    "__Build the trainer image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a725d9b1-3300-4ef8-b188-c8cdfd344988",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb1c067-dc5a-439c-928d-787a05c10969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 5.5 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://zeta-rush-341516_cloudbuild/source/1647974953.128742-e9760e2e00ef43f6922264d02a27a93c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/zeta-rush-341516/locations/global/builds/f3509351-be20-451a-91f4-85e9c0e16ae7].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/f3509351-be20-451a-91f4-85e9c0e16ae7?project=156920671469].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f3509351-be20-451a-91f4-85e9c0e16ae7\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://zeta-rush-341516_cloudbuild/source/1647974953.128742-e9760e2e00ef43f6922264d02a27a93c.tgz#1647974953458121\n",
      "Copying gs://zeta-rush-341516_cloudbuild/source/1647974953.128742-e9760e2e00ef43f6922264d02a27a93c.tgz#1647974953458121...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "7c3b88808835: Already exists\n",
      "382fcf64a9ea: Pulling fs layer\n",
      "d764c2aa40d3: Pulling fs layer\n",
      "90cc2e264020: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "395e65f0ab42: Pulling fs layer\n",
      "9e19ad4dbd7d: Pulling fs layer\n",
      "957c609522d8: Pulling fs layer\n",
      "6a5e2168e631: Pulling fs layer\n",
      "5740bb01bc78: Pulling fs layer\n",
      "be09da654f5c: Pulling fs layer\n",
      "288d40a4f176: Pulling fs layer\n",
      "e2d3eec75c0c: Pulling fs layer\n",
      "3769728eb7d7: Pulling fs layer\n",
      "211e30f752a4: Pulling fs layer\n",
      "ae6a5f7af5b1: Pulling fs layer\n",
      "274bb2dca45b: Pulling fs layer\n",
      "4105864a46df: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "395e65f0ab42: Waiting\n",
      "9e19ad4dbd7d: Waiting\n",
      "3769728eb7d7: Waiting\n",
      "957c609522d8: Waiting\n",
      "6a5e2168e631: Waiting\n",
      "211e30f752a4: Waiting\n",
      "ae6a5f7af5b1: Waiting\n",
      "5740bb01bc78: Waiting\n",
      "be09da654f5c: Waiting\n",
      "288d40a4f176: Waiting\n",
      "e2d3eec75c0c: Waiting\n",
      "274bb2dca45b: Waiting\n",
      "4105864a46df: Waiting\n",
      "382fcf64a9ea: Verifying Checksum\n",
      "382fcf64a9ea: Download complete\n",
      "382fcf64a9ea: Pull complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "395e65f0ab42: Verifying Checksum\n",
      "395e65f0ab42: Download complete\n",
      "90cc2e264020: Verifying Checksum\n",
      "90cc2e264020: Download complete\n",
      "957c609522d8: Verifying Checksum\n",
      "957c609522d8: Download complete\n",
      "9e19ad4dbd7d: Download complete\n",
      "6a5e2168e631: Verifying Checksum\n",
      "6a5e2168e631: Download complete\n",
      "5740bb01bc78: Verifying Checksum\n",
      "5740bb01bc78: Download complete\n",
      "be09da654f5c: Verifying Checksum\n",
      "be09da654f5c: Download complete\n",
      "288d40a4f176: Verifying Checksum\n",
      "288d40a4f176: Download complete\n",
      "e2d3eec75c0c: Verifying Checksum\n",
      "e2d3eec75c0c: Download complete\n",
      "3769728eb7d7: Verifying Checksum\n",
      "3769728eb7d7: Download complete\n",
      "211e30f752a4: Verifying Checksum\n",
      "211e30f752a4: Download complete\n",
      "ae6a5f7af5b1: Verifying Checksum\n",
      "ae6a5f7af5b1: Download complete\n",
      "4105864a46df: Verifying Checksum\n",
      "4105864a46df: Download complete\n",
      "d764c2aa40d3: Verifying Checksum\n",
      "d764c2aa40d3: Download complete\n",
      "274bb2dca45b: Verifying Checksum\n",
      "274bb2dca45b: Download complete\n",
      "d764c2aa40d3: Pull complete\n",
      "90cc2e264020: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "395e65f0ab42: Pull complete\n",
      "9e19ad4dbd7d: Pull complete\n",
      "957c609522d8: Pull complete\n",
      "6a5e2168e631: Pull complete\n",
      "5740bb01bc78: Pull complete\n",
      "be09da654f5c: Pull complete\n",
      "288d40a4f176: Pull complete\n",
      "e2d3eec75c0c: Pull complete\n",
      "3769728eb7d7: Pull complete\n",
      "211e30f752a4: Pull complete\n",
      "ae6a5f7af5b1: Pull complete\n",
      "274bb2dca45b: Pull complete\n",
      "4105864a46df: Pull complete\n",
      "Digest: sha256:5290a56a15cebd867722be8bdfd859ef959ffd14f85979a9fbd80c5c2760c3a1\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 0db22ebb67a2\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 3b3a9e18ff7a\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 5.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 44.3 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 52.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=d401e317d4185cdc1a5b4982496eb7ebd7896a4b9897cdfad75f201a6a83a61f\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=b204c95016da0cc5117fdd46fc664f657ec42d27ca0465f6cb0257bb7cfa1e70\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=16fb90f6f71e83297c17454086c537adc7f2b23713e8fe66f76b44e5982310f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 3b3a9e18ff7a\n",
      " ---> 1b5d66ce3bb2\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 4f3d7c4611d1\n",
      "Removing intermediate container 4f3d7c4611d1\n",
      " ---> 2b0e682e1507\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 3007c5172e8f\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in e8c08b6b82f6\n",
      "Removing intermediate container e8c08b6b82f6\n",
      " ---> a3c80dcb8c33\n",
      "Successfully built a3c80dcb8c33\n",
      "Successfully tagged gcr.io/zeta-rush-341516/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/zeta-rush-341516/trainer_image:latest\n",
      "The push refers to repository [gcr.io/zeta-rush-341516/trainer_image]\n",
      "eae8892dbb45: Preparing\n",
      "4df20e8167f4: Preparing\n",
      "1d4d2acdbc6a: Preparing\n",
      "83a0dd2b9e38: Preparing\n",
      "9638e29d8d24: Preparing\n",
      "b3ab95a574c8: Preparing\n",
      "d1b010151b48: Preparing\n",
      "b80bc089358e: Preparing\n",
      "11bc9b36546a: Preparing\n",
      "43d282ce8d0b: Preparing\n",
      "69fd467ac3a5: Preparing\n",
      "ed4291c31559: Preparing\n",
      "4bf5ae11254c: Preparing\n",
      "0d592bcbe281: Preparing\n",
      "770c4c112e39: Preparing\n",
      "1874048fd290: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7e897a45d8d8: Preparing\n",
      "42826651fb01: Preparing\n",
      "4236d5cafaa0: Preparing\n",
      "68a85fa9d77e: Preparing\n",
      "b3ab95a574c8: Waiting\n",
      "d1b010151b48: Waiting\n",
      "b80bc089358e: Waiting\n",
      "11bc9b36546a: Waiting\n",
      "43d282ce8d0b: Waiting\n",
      "69fd467ac3a5: Waiting\n",
      "ed4291c31559: Waiting\n",
      "4bf5ae11254c: Waiting\n",
      "0d592bcbe281: Waiting\n",
      "770c4c112e39: Waiting\n",
      "1874048fd290: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "7e897a45d8d8: Waiting\n",
      "42826651fb01: Waiting\n",
      "4236d5cafaa0: Waiting\n",
      "68a85fa9d77e: Waiting\n",
      "9638e29d8d24: Mounted from deeplearning-platform-release/base-cpu\n",
      "83a0dd2b9e38: Mounted from deeplearning-platform-release/base-cpu\n",
      "b3ab95a574c8: Mounted from deeplearning-platform-release/base-cpu\n",
      "d1b010151b48: Mounted from deeplearning-platform-release/base-cpu\n",
      "b80bc089358e: Mounted from deeplearning-platform-release/base-cpu\n",
      "eae8892dbb45: Pushed\n",
      "4df20e8167f4: Pushed\n",
      "11bc9b36546a: Mounted from deeplearning-platform-release/base-cpu\n",
      "43d282ce8d0b: Mounted from deeplearning-platform-release/base-cpu\n",
      "69fd467ac3a5: Mounted from deeplearning-platform-release/base-cpu\n",
      "ed4291c31559: Mounted from deeplearning-platform-release/base-cpu\n",
      "4bf5ae11254c: Mounted from deeplearning-platform-release/base-cpu\n",
      "0d592bcbe281: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "770c4c112e39: Mounted from deeplearning-platform-release/base-cpu\n",
      "1874048fd290: Mounted from deeplearning-platform-release/base-cpu\n",
      "42826651fb01: Mounted from deeplearning-platform-release/base-cpu\n",
      "7e897a45d8d8: Mounted from deeplearning-platform-release/base-cpu\n",
      "68a85fa9d77e: Layer already exists\n",
      "4236d5cafaa0: Mounted from deeplearning-platform-release/base-cpu\n",
      "1d4d2acdbc6a: Pushed\n",
      "latest: digest: sha256:91da9bd5a2ef4212a80a372fe072148e4c36b8171977a02ee37c6522013b8ce5 size: 4707\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                           STATUS\n",
      "f3509351-be20-451a-91f4-85e9c0e16ae7  2022-03-22T18:49:13+00:00  2M11S     gs://zeta-rush-341516_cloudbuild/source/1647974953.128742-e9760e2e00ef43f6922264d02a27a93c.tgz  gcr.io/zeta-rush-341516/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009bb50-dbfa-423a-8306-4ca2bd757f1a",
   "metadata": {},
   "source": [
    "__Build the base image for custom components__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79142a2c-2174-4bf2-946c-13dbb3dca85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed222cf-6b85-4d46-a2b9-9b71d2a7c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 242 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://zeta-rush-341516_cloudbuild/source/1647975205.79693-b113f66d2d2b42fdb2deadbfce021e61.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/zeta-rush-341516/locations/global/builds/0a2b90f6-fc53-45ca-b673-93b854c00db9].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/0a2b90f6-fc53-45ca-b673-93b854c00db9?project=156920671469].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0a2b90f6-fc53-45ca-b673-93b854c00db9\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://zeta-rush-341516_cloudbuild/source/1647975205.79693-b113f66d2d2b42fdb2deadbfce021e61.tgz#1647975206122142\n",
      "Copying gs://zeta-rush-341516_cloudbuild/source/1647975205.79693-b113f66d2d2b42fdb2deadbfce021e61.tgz#1647975206122142...\n",
      "/ [1 files][  368.0 B/  368.0 B]                                                \n",
      "Operation completed over 1 objects/368.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "7c3b88808835: Already exists\n",
      "382fcf64a9ea: Pulling fs layer\n",
      "d764c2aa40d3: Pulling fs layer\n",
      "90cc2e264020: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "395e65f0ab42: Pulling fs layer\n",
      "9e19ad4dbd7d: Pulling fs layer\n",
      "957c609522d8: Pulling fs layer\n",
      "6a5e2168e631: Pulling fs layer\n",
      "5740bb01bc78: Pulling fs layer\n",
      "be09da654f5c: Pulling fs layer\n",
      "288d40a4f176: Pulling fs layer\n",
      "e2d3eec75c0c: Pulling fs layer\n",
      "3769728eb7d7: Pulling fs layer\n",
      "211e30f752a4: Pulling fs layer\n",
      "ae6a5f7af5b1: Pulling fs layer\n",
      "274bb2dca45b: Pulling fs layer\n",
      "4105864a46df: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "395e65f0ab42: Waiting\n",
      "9e19ad4dbd7d: Waiting\n",
      "957c609522d8: Waiting\n",
      "6a5e2168e631: Waiting\n",
      "5740bb01bc78: Waiting\n",
      "be09da654f5c: Waiting\n",
      "288d40a4f176: Waiting\n",
      "e2d3eec75c0c: Waiting\n",
      "3769728eb7d7: Waiting\n",
      "211e30f752a4: Waiting\n",
      "ae6a5f7af5b1: Waiting\n",
      "274bb2dca45b: Waiting\n",
      "4105864a46df: Waiting\n",
      "382fcf64a9ea: Verifying Checksum\n",
      "382fcf64a9ea: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "382fcf64a9ea: Pull complete\n",
      "395e65f0ab42: Verifying Checksum\n",
      "395e65f0ab42: Download complete\n",
      "9e19ad4dbd7d: Verifying Checksum\n",
      "9e19ad4dbd7d: Download complete\n",
      "90cc2e264020: Verifying Checksum\n",
      "90cc2e264020: Download complete\n",
      "957c609522d8: Verifying Checksum\n",
      "957c609522d8: Download complete\n",
      "6a5e2168e631: Download complete\n",
      "5740bb01bc78: Verifying Checksum\n",
      "5740bb01bc78: Download complete\n",
      "be09da654f5c: Download complete\n",
      "288d40a4f176: Download complete\n",
      "e2d3eec75c0c: Verifying Checksum\n",
      "e2d3eec75c0c: Download complete\n",
      "3769728eb7d7: Verifying Checksum\n",
      "3769728eb7d7: Download complete\n",
      "211e30f752a4: Download complete\n",
      "ae6a5f7af5b1: Download complete\n",
      "4105864a46df: Verifying Checksum\n",
      "4105864a46df: Download complete\n",
      "274bb2dca45b: Verifying Checksum\n",
      "274bb2dca45b: Download complete\n",
      "d764c2aa40d3: Verifying Checksum\n",
      "d764c2aa40d3: Download complete\n",
      "d764c2aa40d3: Pull complete\n",
      "90cc2e264020: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "395e65f0ab42: Pull complete\n",
      "9e19ad4dbd7d: Pull complete\n",
      "957c609522d8: Pull complete\n",
      "6a5e2168e631: Pull complete\n",
      "5740bb01bc78: Pull complete\n",
      "be09da654f5c: Pull complete\n",
      "288d40a4f176: Pull complete\n",
      "e2d3eec75c0c: Pull complete\n",
      "3769728eb7d7: Pull complete\n",
      "211e30f752a4: Pull complete\n",
      "ae6a5f7af5b1: Pull complete\n",
      "274bb2dca45b: Pull complete\n",
      "4105864a46df: Pull complete\n",
      "Digest: sha256:5290a56a15cebd867722be8bdfd859ef959ffd14f85979a9fbd80c5c2760c3a1\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 0db22ebb67a2\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in ae891be64316\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 5.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 45.7 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 53.6 MB/s eta 0:00:00\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 KB 20.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.8/118.8 KB 19.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2021.10.8)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (6.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.2.1)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 70.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.3.0)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (36.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.6.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 9.0 MB/s eta 0:00:00\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (4.4.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.2/46.2 KB 7.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 KB 12.0 MB/s eta 0:00:00\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.27.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.3.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (3.19.4)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.11.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (59.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.37.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.54.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kfp==0.2.5) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.0.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.2.0)\n",
      "Building wheels for collected packages: kfp, argo-models, tabulate, fire, kfp-server-api, strip-hints, termcolor\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159980 sha256=928b35ac795911adb4d455ce00c4f4966715f468d7b659f79130eccb478102a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57308 sha256=11ebc152f26ebdcb70dc69b816b92ddb34d42467fdbd356bc0c70499639caf32\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23391 sha256=c93881e11069216e571cc4a383a1710cbd721d85d85fdd90bd15f663b440afdc\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=0866cca597f3e89bc397c774773c6642107adfbc00e852404c12a86f2f74aca2\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102465 sha256=7c73be90da4d66c55016f6381eafa78a8143d2edc40a6a23f670db74d2f86b56\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=ed7bbbb5bcd32e99d6f858c6816e12c028b3646c90359c22d527bc0f9d4958e9\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=014a069e774f808c0e0bbc0b5721c5d6446996f40e13c779a94932266a5904e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built kfp argo-models tabulate fire kfp-server-api strip-hints termcolor\n",
      "Installing collected packages: termcolor, tabulate, cloudpickle, urllib3, strip-hints, fire, Deprecated, click, scikit-learn, pandas, kfp-server-api, requests_toolbelt, kubernetes, argo-models, kfp\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 23.3.0\n",
      "    Uninstalling kubernetes-23.3.0:\n",
      "      Successfully uninstalled kubernetes-23.3.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "black 22.1.0 requires click>=8.0.0, but you have click 7.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.4.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests_toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.10 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container ae891be64316\n",
      " ---> b9c1ae3b9f2c\n",
      "Successfully built b9c1ae3b9f2c\n",
      "Successfully tagged gcr.io/zeta-rush-341516/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/zeta-rush-341516/base_image:latest\n",
      "The push refers to repository [gcr.io/zeta-rush-341516/base_image]\n",
      "f680cfb32b18: Preparing\n",
      "83a0dd2b9e38: Preparing\n",
      "9638e29d8d24: Preparing\n",
      "b3ab95a574c8: Preparing\n",
      "d1b010151b48: Preparing\n",
      "b80bc089358e: Preparing\n",
      "11bc9b36546a: Preparing\n",
      "43d282ce8d0b: Preparing\n",
      "69fd467ac3a5: Preparing\n",
      "ed4291c31559: Preparing\n",
      "4bf5ae11254c: Preparing\n",
      "0d592bcbe281: Preparing\n",
      "770c4c112e39: Preparing\n",
      "1874048fd290: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7e897a45d8d8: Preparing\n",
      "42826651fb01: Preparing\n",
      "4236d5cafaa0: Preparing\n",
      "68a85fa9d77e: Preparing\n",
      "4bf5ae11254c: Waiting\n",
      "0d592bcbe281: Waiting\n",
      "770c4c112e39: Waiting\n",
      "1874048fd290: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "7e897a45d8d8: Waiting\n",
      "42826651fb01: Waiting\n",
      "4236d5cafaa0: Waiting\n",
      "68a85fa9d77e: Waiting\n",
      "b80bc089358e: Waiting\n",
      "11bc9b36546a: Waiting\n",
      "43d282ce8d0b: Waiting\n",
      "69fd467ac3a5: Waiting\n",
      "ed4291c31559: Waiting\n",
      "b3ab95a574c8: Layer already exists\n",
      "d1b010151b48: Layer already exists\n",
      "83a0dd2b9e38: Layer already exists\n",
      "9638e29d8d24: Layer already exists\n",
      "11bc9b36546a: Layer already exists\n",
      "b80bc089358e: Layer already exists\n",
      "69fd467ac3a5: Layer already exists\n",
      "43d282ce8d0b: Layer already exists\n",
      "ed4291c31559: Layer already exists\n",
      "770c4c112e39: Layer already exists\n",
      "0d592bcbe281: Layer already exists\n",
      "4bf5ae11254c: Layer already exists\n",
      "42826651fb01: Layer already exists\n",
      "1874048fd290: Layer already exists\n",
      "7e897a45d8d8: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "68a85fa9d77e: Layer already exists\n",
      "4236d5cafaa0: Layer already exists\n",
      "f680cfb32b18: Pushed\n",
      "latest: digest: sha256:80ccb690b3b6c19cc4c43140a081eeed6f8a5aeef4d34a03d749e40f1e62c32a size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                        STATUS\n",
      "0a2b90f6-fc53-45ca-b673-93b854c00db9  2022-03-22T18:53:26+00:00  2M20S     gs://zeta-rush-341516_cloudbuild/source/1647975205.79693-b113f66d2d2b42fdb2deadbfce021e61.tgz  gcr.io/zeta-rush-341516/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fcea5-4b55-4b39-9783-492f06338b4d",
   "metadata": {},
   "source": [
    "__Compile the pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f8c3d-cb17-4db3-b68c-87982025f9ad",
   "metadata": {},
   "source": [
    "We can compile the DSL using an API from the `KFP SDK` or using the `KFP` compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb9f4a1-cabf-44b3-8355-557e2e01fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/zeta-rush-341516/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/zeta-rush-341516/trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc79b41f-1be1-4030-9e5e-a35716ba7936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/components/_components.py:198: FutureWarning: Container component must specify command to be compatible with KFP v2 compatible mode and emissary executor, which will be the default executor for KFP v2.https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92916964-e4b2-410a-b69a-1ce3be904dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: covertype-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11, pipelines.kubeflow.org/pipeline_compilation_time: '2022-03-22T19:23:32.606049',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Covertype classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"source_table_name\"}, {\"name\": \"gcs_root\"}, {\"name\":\n",
      "      \"dataset_id\"}, {\"name\": \"evaluation_metric_name\"}, {\"name\": \"evaluation_metric_threshold\"},\n",
      "      {\"name\": \"model_id\"}, {\"name\": \"version_id\"}, {\"name\": \"replace_existing_version\"},\n"
     ]
    }
   ],
   "source": [
    "!head covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92419e5-b8eb-4c55-8193-bd8129a03bac",
   "metadata": {},
   "source": [
    "__Deploy the pipeline package__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c52b6f9-b85c-4b76-b015-88ec9f9d930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Details\n",
      "------------------\n",
      "Pipeline ID  36237948-da8c-4cfb-b1f6-f5a45a586c9a\n",
      "Name         covertype_continuous_training\n",
      "Description\n",
      "Uploaded at  2022-03-22T19:24:40+00:00\n",
      "Version ID   36237948-da8c-4cfb-b1f6-f5a45a586c9a\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_continuous_training'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3b604-2d5c-480a-bf1b-92489d1cd97d",
   "metadata": {},
   "source": [
    "Submitting pipeline runs\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62677d7f-68b5-46b5-a956-d694500a2ddf",
   "metadata": {},
   "source": [
    "We List the pipelines in AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae7e723-8315-46b8-afa8-7bf00241b14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                           | Uploaded at               |\n",
      "+======================================+================================================+===========================+\n",
      "| 36237948-da8c-4cfb-b1f6-f5a45a586c9a | covertype_continuous_training                  | 2022-03-22T19:24:40+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 4db0f015-4ba2-4f2d-85b4-668674c5f0be | [Tutorial] V2 lightweight Python components    | 2022-03-22T18:40:06+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 730bf3ee-112b-4235-a90a-d456543dd1ed | [Tutorial] DSL - Control structures            | 2022-03-22T18:40:05+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| bace33ba-f5bb-4b11-a9e6-37017031063c | [Tutorial] Data passing in python components   | 2022-03-22T18:40:03+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 92af24e4-f01f-42a5-a6f5-40df4ba1bfda | [Demo] TFX - Taxi tip prediction model trainer | 2022-03-22T18:40:02+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| d87a63f4-fa2f-4458-b204-fa0d7beb808d | [Demo] XGBoost - Iterative model training      | 2022-03-22T18:40:01+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327c0ee-949d-42ec-a522-cd174804b15d",
   "metadata": {},
   "source": [
    "__Submit a run__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78cd48-6579-459f-89aa-c94ba9d7cc58",
   "metadata": {},
   "source": [
    "The ID of the covertype_continuous_training pipeline we uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470cfaf6-d044-45cf-80ad-c5b5ad143d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='36237948-da8c-4cfb-b1f6-f5a45a586c9a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01fbc5f0-76e8-480f-bdd2-9802395b0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb51e24b-56fe-4667-ab0a-9962117ed0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment Covertype_Classifier_Training.\n",
      "+--------------------------------------+---------+----------+---------------------------+--------------------------------------+\n",
      "| run id                               | name    | status   | created at                | experiment id                        |\n",
      "+======================================+=========+==========+===========================+======================================+\n",
      "| b0270dbe-6132-40f1-a6bf-3dd274169bc8 | Run_001 |          | 2022-03-22T19:28:05+00:00 | acdce9ad-ec41-4003-b843-a8e6e82c7b37 |\n",
      "+--------------------------------------+---------+----------+---------------------------+--------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb229c-0668-474b-8979-eb1040239ffa",
   "metadata": {},
   "source": [
    "__Monitoring the run__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48295912-6477-46cf-a80d-c90eeb37c805",
   "metadata": {},
   "source": [
    "We can monitor the run using KFP UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fbccd-b89a-4f06-a3f8-2c556a0004a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
